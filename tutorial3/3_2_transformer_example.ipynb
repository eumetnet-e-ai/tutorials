{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1724522549414,
     "user": {
      "displayName": "Roland Potthast",
      "userId": "09141136587533247770"
     },
     "user_tz": -120
    },
    "id": "L3rPyneK69Zy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1724523434987,
     "user": {
      "displayName": "Roland Potthast",
      "userId": "09141136587533247770"
     },
     "user_tz": -120
    },
    "id": "3S3MIiE967Vz",
    "outputId": "b9fb14d4-1064-4d07-e60e-1add74b1a4fa"
   },
   "outputs": [],
   "source": [
    "# Example Dataset\n",
    "sentences = [\n",
    "    \"The sky is clear, and the sun is shining brightly.\",\n",
    "    \"Tomorrow's forecast predicts a chance of thunderstorms.\",\n",
    "    \"The temperature is expected to drop below freezing tonight.\",\n",
    "    \"The weather is perfect for a day at the beach.\",\n",
    "    \"Strong winds are causing power outages across the region.\",\n",
    "    \"A hurricane is approaching the coastline, and residents are advised to evacuate.\",\n",
    "    \"There is a severe weather warning in effect until midnight.\",\n",
    "    \"The sunset painted the sky with hues of orange and pink.\",\n",
    "    \"The heatwave has broken temperature records this year.\",\n",
    "    \"It's a cloudy day with a chance of light showers in the afternoon.\",\n",
    "    \"The weather has been unpredictable lately, changing from sunny to rainy within hours.\",\n",
    "    \"The spring blossoms are early this year due to mild weather.\",\n",
    "    \"People are enjoying outdoor concerts as the nights get warmer.\",\n",
    "    \"A warm breeze carried the scent of blooming flowers through the air.\",\n",
    "    \"A heat advisory has been issued for the upcoming days.\",\n",
    "    \"The local weather station reported record high temperatures today.\",\n",
    "    \"A cool breeze is a welcome relief from the afternoon sun.\",\n",
    "    \"Unexpected weather changes have become a common theme this year.\",\n",
    "    \"The windchill factor makes it feel much colder outside.\",\n",
    "]\n",
    "\n",
    "# Build vocabulary mapping words to IDs\n",
    "def build_vocab(sentences):\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    index = 2\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.lower().split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "vocab_size = len(vocab)\n",
    "padding_idx = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_sentence(sentence, vocab):\n",
    "    return [vocab.get(word.lower(), vocab[\"<unk>\"]) for word in sentence.split()]\n",
    "\n",
    "# Padding function\n",
    "def pad_sequence(seq, max_len, pad_value=0):\n",
    "    return seq + [pad_value] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "        self.data = [tokenize_sentence(sentence, vocab) for sentence in sentences]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        x = seq[:-1]  # Input sequence\n",
    "        y = seq[1:]   # Target sequence (shifted by one)\n",
    "        x_padded = pad_sequence(x, self.max_len)\n",
    "        y_padded = pad_sequence(y, self.max_len)\n",
    "        return torch.tensor(x_padded, dtype=torch.long), torch.tensor(y_padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_len, padding_idx):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        src_pad_mask = (src == padding_idx).to(src.device)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src.transpose(0, 1), mask=src_mask, src_key_padding_mask=src_pad_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200], Loss: 4.2328\n",
      "Epoch [10/200], Loss: 3.4469\n",
      "Epoch [15/200], Loss: 2.7120\n",
      "Epoch [20/200], Loss: 2.0709\n",
      "Epoch [25/200], Loss: 1.5228\n",
      "Epoch [30/200], Loss: 1.1387\n",
      "Epoch [35/200], Loss: 0.9051\n",
      "Epoch [40/200], Loss: 0.6911\n",
      "Epoch [45/200], Loss: 0.5742\n",
      "Epoch [50/200], Loss: 0.4863\n",
      "Epoch [55/200], Loss: 0.4248\n",
      "Epoch [60/200], Loss: 0.3807\n",
      "Epoch [65/200], Loss: 0.3357\n",
      "Epoch [70/200], Loss: 0.2951\n",
      "Epoch [75/200], Loss: 0.2873\n",
      "Epoch [80/200], Loss: 0.2627\n",
      "Epoch [85/200], Loss: 0.2357\n",
      "Epoch [90/200], Loss: 0.2160\n",
      "Epoch [95/200], Loss: 0.2161\n",
      "Epoch [100/200], Loss: 0.1970\n",
      "Epoch [105/200], Loss: 0.2107\n",
      "Epoch [110/200], Loss: 0.1925\n",
      "Epoch [115/200], Loss: 0.1997\n",
      "Epoch [120/200], Loss: 0.1923\n",
      "Epoch [125/200], Loss: 0.1806\n",
      "Epoch [130/200], Loss: 0.1879\n",
      "Epoch [135/200], Loss: 0.2028\n",
      "Epoch [140/200], Loss: 0.1772\n",
      "Epoch [145/200], Loss: 0.1941\n",
      "Epoch [150/200], Loss: 0.1680\n",
      "Epoch [155/200], Loss: 0.1955\n",
      "Epoch [160/200], Loss: 0.1798\n",
      "Epoch [165/200], Loss: 0.1839\n",
      "Epoch [170/200], Loss: 0.1731\n",
      "Epoch [175/200], Loss: 0.1766\n",
      "Epoch [180/200], Loss: 0.1682\n",
      "Epoch [185/200], Loss: 0.1688\n",
      "Epoch [190/200], Loss: 0.1836\n",
      "Epoch [195/200], Loss: 0.1751\n",
      "Epoch [200/200], Loss: 0.1599\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "max_len = 15\n",
    "batch_size = 2\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dim_feedforward = 128\n",
    "num_epochs = 200\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TextDataset(sentences, vocab, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, max_len, padding_idx)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if (epoch%5==0):\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "The weather is perfect for a day at the beach.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text generation function\n",
    "def generate_text(model, vocab, start_text, max_len):\n",
    "    model.eval()\n",
    "    words = start_text.lower().split()\n",
    "    input_ids = [vocab.get(word, vocab[\"<unk>\"]) for word in words]\n",
    "    generated = words.copy()\n",
    "    generated[0]=generated[0].capitalize()\n",
    "    input_seq = torch.tensor([pad_sequence(input_ids, max_len)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - len(input_ids)):\n",
    "            output = model(input_seq)\n",
    "            next_token_logits = output[0, len(generated) - 1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            next_word = [word for word, idx in vocab.items() if idx == next_token_id][0]\n",
    "            generated.append(next_word)\n",
    "            input_seq[0, len(generated) - 1] = next_token_id\n",
    "            if next_token_id == vocab[\"<pad>\"] or next_token_id == vocab[\"<unk>\"] or any([s in next_word for s in {'.', '!', '?'}]):\n",
    "                break\n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Generate text\n",
    "start_text = \"The weather\"\n",
    "words=start_text.lower().split()\n",
    "generated_text = generate_text(model, vocab, start_text, max_len)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqzrZ2Ox1pYCh9SvUgAsLy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
